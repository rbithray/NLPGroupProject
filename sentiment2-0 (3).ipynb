{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-08T19:41:58.591734Z",
     "iopub.status.busy": "2025-01-08T19:41:58.591435Z",
     "iopub.status.idle": "2025-01-08T19:41:58.905938Z",
     "shell.execute_reply": "2025-01-08T19:41:58.905090Z",
     "shell.execute_reply.started": "2025-01-08T19:41:58.591709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "#Daadwerkelijke comment in \"body\"\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:41:58.907420Z",
     "iopub.status.busy": "2025-01-08T19:41:58.906954Z",
     "iopub.status.idle": "2025-01-08T19:42:07.218397Z",
     "shell.execute_reply": "2025-01-08T19:42:07.217332Z",
     "shell.execute_reply.started": "2025-01-08T19:41:58.907394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\jiswi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jiswi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jiswi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->vaderSentiment) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jiswi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jiswi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jiswi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->vaderSentiment) (2024.8.30)\n",
      "Requirement already satisfied: ndjson in c:\\users\\jiswi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "!pip install ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:49:25.017066Z",
     "iopub.status.busy": "2025-01-08T19:49:25.016704Z",
     "iopub.status.idle": "2025-01-08T19:49:25.849746Z",
     "shell.execute_reply": "2025-01-08T19:49:25.848827Z",
     "shell.execute_reply.started": "2025-01-08T19:49:25.017034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 108342: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load the NDJSON file containing data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dataset_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 21\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m \u001b[43mndjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Parses the entire NDJSON file into a Python list\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Example: Extract 'body' field if present\u001b[39;00m\n\u001b[0;32m     24\u001b[0m bodies \u001b[38;5;241m=\u001b[39m [entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m json_data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\ndjson\\api.py:9\u001b[0m, in \u001b[0;36mload\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      8\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m, Decoder)\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 108342: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ndjson\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the dataset path and the ndjson file path\n",
    "dataset_path = 'data/comments.ndjson'\n",
    "ndjson_file_path = os.path.join(dataset_path, 'artificialinteligence_comments.ndjson')\n",
    "\n",
    "# Load the NDJSON file containing data\n",
    "with open(dataset_path, 'r') as f:\n",
    "    json_data = ndjson.load(f)  # Parses the entire NDJSON file into a Python list\n",
    "\n",
    "# Example: Extract 'body' field if present\n",
    "bodies = [entry['body'] for entry in json_data if 'body' in entry]\n",
    "\n",
    "# Print the first few bodies\n",
    "print(bodies[:5])  # Adjust the slicing as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier moet nog code komen om alles met regulation eruit te krijgen, Lijst maken met hoe en wat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:42:08.663359Z",
     "iopub.status.busy": "2025-01-08T19:42:08.662930Z",
     "iopub.status.idle": "2025-01-08T19:42:09.666984Z",
     "shell.execute_reply": "2025-01-08T19:42:09.666096Z",
     "shell.execute_reply.started": "2025-01-08T19:42:08.663335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file data/comments.ndjson\\artificialinteligence_comments.ndjson does not exist.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Filter the entries that match at least one keyword in their body\u001b[39;00m\n\u001b[0;32m     37\u001b[0m matched_entries \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 38\u001b[0m     entry \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mjson_data\u001b[49m \n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m entry \u001b[38;5;129;01mand\u001b[39;00m regex\u001b[38;5;241m.\u001b[39msearch(entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     40\u001b[0m ]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Write matched entries to an NDJSON file\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json_data' is not defined"
     ]
    }
   ],
   "source": [
    "output_dir = 'output'  # Directory to store output files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "output_file_path = os.path.join(output_dir, 'matched_entries.ndjson')\n",
    "csv_file_path = os.path.join(output_dir, 'matched_entries.csv')\n",
    "first_100_ndjson_path = os.path.join(output_dir, 'first_100_matched_entries.ndjson')\n",
    "\n",
    "# Define the regex pattern using the initial set of keywords\n",
    "pattern = (\n",
    "    r\"\\b(?:\"\n",
    "    r\"Regulat(?:ion|ions|ory)|\"    # matches \"Regulation\", \"Regulations\", \"Regulatory\"\n",
    "    r\"Law(?:s)?|\"                  # matches \"Law\" or \"Laws\"\n",
    "    r\"Government|\"\n",
    "    r\"Polic(?:y|ies)\"              # matches \"Policy\" or \"Policies\"\n",
    "    r\")\\b\"\n",
    ")\n",
    "\n",
    "# Compile the regex pattern with case-insensitive flag\n",
    "try:\n",
    "    regex = re.compile(pattern, re.IGNORECASE)\n",
    "except re.error as e:\n",
    "    print(f\"Regex compilation error: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the NDJSON data with error handling\n",
    "try:\n",
    "    with open(ndjson_file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = ndjson.load(f)  # Parses the entire NDJSON file into a Python list of dictionaries\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {ndjson_file_path} does not exist.\")\n",
    "    exit(1)\n",
    "except ndjson.JSONDecodeError:\n",
    "    print(f\"Error: The file {ndjson_file_path} is not a valid NDJSON file.\")\n",
    "    exit(1)\n",
    "\n",
    "# Filter the entries that match at least one keyword in their body\n",
    "matched_entries = [\n",
    "    entry for entry in json_data \n",
    "    if 'body' in entry and regex.search(entry['body'])\n",
    "]\n",
    "\n",
    "# Write matched entries to an NDJSON file\n",
    "try:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        ndjson.dump(matched_entries, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to {output_file_path}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create a DataFrame from the matched entries\n",
    "df = pd.DataFrame(matched_entries)\n",
    "\n",
    "# Save the matched entries to a CSV file\n",
    "try:\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to {csv_file_path}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Save the first 100 matched entries to another NDJSON file\n",
    "first_100_entries = matched_entries[:100]\n",
    "try:\n",
    "    with open(first_100_ndjson_path, 'w', encoding='utf-8') as f:\n",
    "        ndjson.dump(first_100_entries, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to {first_100_ndjson_path}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Found {len(matched_entries)} bodies containing at least one regulatory/policy keyword.\")\n",
    "print(\"Sample matched bodies:\", [e['body'] for e in matched_entries[:3]])\n",
    "print(\"Matched entries saved to:\")\n",
    "print(f\"  NDJSON: {output_file_path}\")\n",
    "print(f\"  CSV: {csv_file_path}\")\n",
    "print(f\"  First 100 NDJSON: {first_100_ndjson_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:42:09.668015Z",
     "iopub.status.busy": "2025-01-08T19:42:09.667814Z",
     "iopub.status.idle": "2025-01-08T19:42:09.831085Z",
     "shell.execute_reply": "2025-01-08T19:42:09.830378Z",
     "shell.execute_reply.started": "2025-01-08T19:42:09.667997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the Sentiment Intensity Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(bodies, columns=['body'])\n",
    "\n",
    "matched_entries = []\n",
    "for entry in json_data:\n",
    "    if 'body' in entry and regex.search(entry['body']):\n",
    "        # Safely get 'id' if it exists\n",
    "        _id = entry['id'] if 'id' in entry else None\n",
    "        matched_entries.append((_id, entry['body']))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Create a DataFrame with 'id' and 'body' columns\n",
    "# -------------------------------------------------------------------\n",
    "df_matched = pd.DataFrame(matched_entries, columns=['id', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:42:09.832709Z",
     "iopub.status.busy": "2025-01-08T19:42:09.832340Z",
     "iopub.status.idle": "2025-01-08T19:42:10.365415Z",
     "shell.execute_reply": "2025-01-08T19:42:10.364543Z",
     "shell.execute_reply.started": "2025-01-08T19:42:09.832677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Comments: 177\n",
      "Negative Comments: 80\n",
      "Neutral Comments: 37\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for each sentiment\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "neutral_count = 0\n",
    "\n",
    "# Analyze each text and classify\n",
    "for text in df_matched['body']:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Classify based on compound score\n",
    "    if scores['compound'] > 0.05:\n",
    "        sentiment = \"Positive\"\n",
    "        positive_count += 1\n",
    "    elif scores['compound'] < -0.05:\n",
    "        sentiment = \"Negative\"\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "        neutral_count += 1\n",
    "\n",
    "# Output the counts of each sentiment\n",
    "print(f\"Positive Comments: {positive_count}\")\n",
    "print(f\"Negative Comments: {negative_count}\")\n",
    "print(f\"Neutral Comments: {neutral_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:49:48.598650Z",
     "iopub.status.busy": "2025-01-08T19:49:48.598307Z",
     "iopub.status.idle": "2025-01-08T19:49:55.988810Z",
     "shell.execute_reply": "2025-01-08T19:49:55.988065Z",
     "shell.execute_reply.started": "2025-01-08T19:49:48.598624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Parameters: {'classifier__alpha': 0.5, 'vectorizer__max_features': 5000, 'vectorizer__ngram_range': (1, 1)}\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.67      0.56       290\n",
      "     neutral       0.38      0.29      0.33       290\n",
      "    positive       0.62      0.52      0.57       290\n",
      "\n",
      "    accuracy                           0.49       870\n",
      "   macro avg       0.49      0.49      0.48       870\n",
      "weighted avg       0.49      0.49      0.48       870\n",
      "\n",
      "\n",
      "Summary of Test Predictions:\n",
      "Positive Comments: 243\n",
      "Negative Comments: 408\n",
      "Neutral Comments: 219\n"
     ]
    }
   ],
   "source": [
    "# Improved Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = text.lower().split()\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load the datasets from GitHub\n",
    "train_url = \"https://raw.githubusercontent.com/tyqiangz/multilingual-sentiment-datasets/main/data/english/train.tsv\"\n",
    "valid_url = \"https://raw.githubusercontent.com/tyqiangz/multilingual-sentiment-datasets/main/data/english/valid.tsv\"\n",
    "test_url = \"https://raw.githubusercontent.com/tyqiangz/multilingual-sentiment-datasets/main/data/english/test.tsv\"\n",
    "\n",
    "df_train = pd.read_csv(train_url, sep=\"\\t\", header=0)\n",
    "df_valid = pd.read_csv(valid_url, sep=\"\\t\", header=0)\n",
    "df_test = pd.read_csv(test_url, sep=\"\\t\", header=0)\n",
    "\n",
    "# Check and remove rows with NaN values for all datasets\n",
    "df_train.dropna(subset=[\"text\", \"label\"], inplace=True)\n",
    "df_valid.dropna(subset=[\"text\", \"label\"], inplace=True)\n",
    "df_test.dropna(subset=[\"text\", \"label\"], inplace=True)\n",
    "\n",
    "# Ensure that labels are strings for consistency\n",
    "df_train[\"label\"] = df_train[\"label\"].astype(str)\n",
    "df_valid[\"label\"] = df_valid[\"label\"].astype(str)\n",
    "df_test[\"label\"] = df_test[\"label\"].astype(str)\n",
    "\n",
    "# Preprocess the text data\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(preprocess_text)\n",
    "df_valid[\"text\"] = df_valid[\"text\"].apply(preprocess_text)\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Combine train and valid for grid search\n",
    "X_combined = pd.concat([df_train[\"text\"], df_valid[\"text\"]])\n",
    "y_combined = pd.concat([df_train[\"label\"], df_valid[\"label\"]])\n",
    "\n",
    "# Define parameter grid for TfidfVectorizer and MultinomialNB\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vectorizer__max_features': [3000, 5000, 7000],\n",
    "    'classifier__alpha': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),  # Bag-of-Words extractor\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_combined, y_combined)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "X_test_tfidf = best_model.named_steps['vectorizer'].transform(df_test[\"text\"])\n",
    "y_test_pred = best_model.named_steps['classifier'].predict(X_test_tfidf)\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(df_test[\"label\"], y_test_pred, zero_division=0))\n",
    "\n",
    "# Provide a summary of test predictions\n",
    "positive_count = (y_test_pred == \"positive\").sum()\n",
    "negative_count = (y_test_pred == \"negative\").sum()\n",
    "neutral_count = (y_test_pred == \"neutral\").sum()\n",
    "\n",
    "print(f\"\\nSummary of Test Predictions:\")\n",
    "print(f\"Positive Comments: {positive_count}\")\n",
    "print(f\"Negative Comments: {negative_count}\")\n",
    "print(f\"Neutral Comments: {neutral_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:50:01.851391Z",
     "iopub.status.busy": "2025-01-08T19:50:01.851057Z",
     "iopub.status.idle": "2025-01-08T19:50:01.885246Z",
     "shell.execute_reply": "2025-01-08T19:50:01.884518Z",
     "shell.execute_reply.started": "2025-01-08T19:50:01.851363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Matched Predictions:\n",
      "Positive Comments: 15\n",
      "Negative Comments: 227\n",
      "Neutral Comments: 52\n"
     ]
    }
   ],
   "source": [
    "# Transform the body text using the vectorizer from the best model\n",
    "X_matched_tfidf = best_model.named_steps['vectorizer'].transform(df_matched[\"body\"])\n",
    "# Predict labels using the classifier from the best model\n",
    "df_matched[\"predicted_label\"] = best_model.named_steps['classifier'].predict(X_matched_tfidf)\n",
    "# print(\"\\nPredicted Labels for df_matched:\")\n",
    "# print(df_matched)\n",
    "\n",
    "# Summary of matched predictions\n",
    "positive_matched = (df_matched[\"predicted_label\"] == \"positive\").sum()\n",
    "negative_matched = (df_matched[\"predicted_label\"] == \"negative\").sum()\n",
    "neutral_matched = (df_matched[\"predicted_label\"] == \"neutral\").sum()\n",
    "\n",
    "print(f\"\\nSummary of Matched Predictions:\")\n",
    "print(f\"Positive Comments: {positive_matched}\")\n",
    "print(f\"Negative Comments: {negative_matched}\")\n",
    "print(f\"Neutral Comments: {neutral_matched}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 22169,
     "sourceId": 30047,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6272889,
     "sourceId": 10159141,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
